Add and use Tasks to better manage work coordination

Status: Proposed

Problem: We need a way to track the state of ongoing/upcoming jobs in
ADI, that will better enable us to

- reason about the status of a job (done, waiting for something else,
  error, lost to the ether)

- understand and act on information about the relationships between
  jobs (ie, job dependencies)

Context:

We're starting to outgrow the existing implementation of
doing asynchronous work between various microservices.
The main interaction right now is between the main server,
which handles API requests and serving up (and servicing
requests from) the browser client, and the python-workers.

We've reduced some of the dependencies between the workers
and the servers in terms of knowledge they have to both
have. The workers now operate with the need for far less
knowledge of the overall system: needing to only know that
something (usually running a script) needs to be done on
certain input files (usually in object storage) producing
certain output files. It does that work and sends a message
back saying it's done (if there were no errors), along
with other useful information (for example, if it's figured
out column names from a CSV transformation, it will send that
information back).

There are some areas where this simple system, involving
only messages back and forth on a message queue, starts to
break down. Let's look at some of these cases:

1. When considering a potential chain of tasks that need to
be run one after the other, there's currently no way to tell
when one has finished and the next one can run. We're operating
right now under a simple request/response model. We make the
request and expect to hear back, but the system isn't smart
enough to know: "I have to still run all these other tasks
before I'm truly done this initial one."

We see this currently with dataset generation tasks. If a
computed dataset depends on another computed dataset, that
one potentially needs to be generated. Because we don't
have anything in place to coordinate starting work after
some other piece of work has finished, we end up having
to figure out what tasks need to be done and run them
sequentially all at once from the same process. If we only
ran one task, the user would effectively have to do that
coordination, waiting for one task to complete and then
starting the next one.

2. When information gets passed back from one of the
worker processes, we have to assume a single place that's
interested in it. So far, that's been a dataset. However,
again we run into issues where our generation task, when
it ends up operating on more than one dataset, makes this
quirky. Specifically, the generation task might discover
column names when it processes structured data. Those
column names are useful metadata, so we want to get that
metadata back to the server and process it. Given that
our generation task needs to process everything at once,
the metadata comes back all at once. It doesn't make sense
for a single dataset to be the receiver of the information
used to update a bunch of datasets, so we end up making
a special case for it in the message queue. We also end
up waiting for all datasets to be updated before updating
their information.

3. Updates coming back from the message queue aren't really
secure at the moment. We're communicating over a private
network between the workers and server, so we don't have 
to worry about things like encryption. However, we will have
to consider that we can't really trust the worker tasks once
actual users end up on the system. Right now, anyone in the
workers can send any message back to the server. If they
knew enough about the system, they could arbitrarily update
dataset metadata. Tying any updates that may come back to a
specific mediating task reduces the size of that attack
vector (the task could verify that the dataset being updated
is actually tied to it in some way). It also makes it easier
for us to add various security measures (for example, a token
that gets passed when the work starts that we expect to get
back to verify).


Alternatives:

1. Build out the concept of "tasks" ourselves in ADI.

Pros:

This is a fairly natural next step. A graph database also makes
many things about tracking work fairly natural. Tasks can 
establish relations directly to the entities they're concerned
with. It's really easy to model a linked list, so it's really
easy to know what the next task is and when you have no more
left. It's also easy to model multiple dependent tasks and tell
when all of them have been completed.

Adding tasks and refactoring the code to use them should be
fairly minimal. Given the control over every part, we can tweak
them to be exactly what we need.

Cons:

It's going to take time for us to build and maintain our own task/job
implementation.

We're turning this more into a job system. The more we go towards
that, the more we have to question whether or not we should just
grab one off the shelf. The advantage of being able to specialize
this to our needs may eventually turn into a liability where we
couldn't use an off the shelf job system. We can mitigate this a
bit by keeping an eye on how regular job systems operate and making
sure to not stray too far from that.

If we're careful to add the Task abstraction correctly, it would set
ups up better to use another job system even if we ended up going that
route later.

2. Adopting an existing job queue that supports multiple
languages.

Pros:

A long standing system that has been developed for the sole purpose
of job scheduling will have had more thought put into it than we
could ever hope to give. These should be more robust and may have
considered important aspects of job scheduling that we haven't.

We could likely remove code. It's always good when you can remove
code.

Cons:

We expect to have to support at least python-based and nodejs based
pieces communicating. But we've always intended to be able to support
doing transformations in other languages as well. So for actual job
systems, the most appealing are ones that aren't tied to a specific
language. This does limit the options available, and many of these
seem to be focused on communicating information to the worker, but
not sending much back, besides whether or not the job was successful.

If we were to implement a 3rd party job system, we'd also need to 
take time to understand its quirks, set up infrastructure it needs 
(many use Redis to store persistent pieces, which means we'd need to 
be adding some extra logic to associate things with our main data 
model in the graph database). It may do much more than we actually
need and would likely take longer to implement than the first option.

Delaying having a task system available is causing us to push adhoc
fixes/workaround downsteam.

Decision:

Implement option 1:

- Build out a tasks implementation in ADI ourselves.

- Minimize the amount of time we spend build it

- Ensure we document it, and have a clear separation of concerns with
  our tasks, in order to make it easier to swap out later.

- Limit scope to simple queuing of work and getting a response that
  it's done with results.
